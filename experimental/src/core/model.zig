const std = @import("std");
const Allocator = std.mem.Allocator;
const Tensor = @import("tensor.zig").Tensor;
const Shape = @import("tensor.zig").Shape;
const Transformer = @import("transformer.zig").Transformer;
const Tokenizer = @import("tokenizer.zig").Tokenizer;
const Backend = @import("backend.zig").Backend;
const CoreError = @import("root.zig").CoreError;

pub const ModelError = CoreError || error{
    InvalidModelFile,
    UnsupportedModelVersion,
    CorruptedWeights,
    MissingTokenizer,
};

/// Model configuration matching DeepSeek V3 architecture
pub const ModelConfig = struct {
    // Model dimensions
    vocab_size: u32,
    hidden_size: u32,
    intermediate_size: u32,
    num_hidden_layers: u32,
    num_attention_heads: u32,
    num_key_value_heads: u32,
    max_position_embeddings: u32,
    
    // MoE configuration
    num_experts: u32,
    num_experts_per_token: u32,
    expert_capacity: u32,
    
    // Multi-head Latent Attention (MLA) config
    qk_nope_head_dim: u32,
    qk_rope_head_dim: u32,
    v_head_dim: u32,
    qk_rope_base: f32,
    
    // Activation function
    hidden_act: []const u8, // "swiglu" for DeepSeek V3
    
    // Normalization
    rms_norm_eps: f32,
    
    // Quantization settings
    use_fp16: bool,
    use_bf16: bool,
    
    pub fn deepseekV3Default() ModelConfig {
        return ModelConfig{
            .vocab_size = 129280,
            .hidden_size = 7168,
            .intermediate_size = 18432,
            .num_hidden_layers = 61,
            .num_attention_heads = 128,
            .num_key_value_heads = 128,
            .max_position_embeddings = 32768,
            .num_experts = 256,
            .num_experts_per_token = 8,
            .expert_capacity = 64,
            .qk_nope_head_dim = 128,
            .qk_rope_head_dim = 64,
            .v_head_dim = 128,
            .qk_rope_base = 10000.0,
            .hidden_act = "swiglu",
            .rms_norm_eps = 1e-6,
            .use_fp16 = false,
            .use_bf16 = true,
        };
    }
};

/// Model information
pub const ModelInfo = struct {
    name: []const u8,
    version: []const u8,
    config: ModelConfig,
    num_parameters: u64,
    memory_usage: u64,
};

/// DeepSeek V3 Model
pub const Model = struct {
    config: ModelConfig,
    transformer: Transformer,
    tokenizer: Tokenizer,
    backend: Backend,
    allocator: Allocator,
    
    // Embedding layers
    embed_tokens: Tensor,
    embed_positions: ?Tensor,
    
    // Output layers
    lm_head: Tensor,
    norm: Tensor,
    
    const Self = @This();
    
    /// Load model from file path
    pub fn loadFromPath(allocator: Allocator, path: []const u8, backend: Backend) !Self {
        std.log.info("Loading DeepSeek V3 model from: {s}", .{path});
        
        // TODO: Implement model loading from file
        // For now, create a default model
        return loadDefault(allocator, backend);
    }
    
    /// Load default/demo model
    pub fn loadDefault(allocator: Allocator, backend: Backend) !Self {
        const config = ModelConfig.deepseekV3Default();
        
        std.log.info("Creating default DeepSeek V3 model...", .{});
        std.log.info("  Hidden size: {}", .{config.hidden_size});
        std.log.info("  Layers: {}", .{config.num_hidden_layers});
        std.log.info("  Experts: {}", .{config.num_experts});
        std.log.info("  Vocab size: {}", .{config.vocab_size});
        
        // Initialize transformer
        const transformer = try Transformer.init(allocator, config, backend);
        
        // Initialize tokenizer
        const tokenizer = try Tokenizer.init(allocator, config.vocab_size);
        
        // Initialize embedding layers
        const embed_shape = Shape.init(&[_]u32{ config.vocab_size, config.hidden_size });
        var embed_tokens = try Tensor.init(allocator, embed_shape, .f32);
        
        // Initialize with random values (in real implementation, load from weights)
        try initializeEmbedding(&embed_tokens);
        
        // Output projection
        const lm_head_shape = Shape.init(&[_]u32{ config.hidden_size, config.vocab_size });
        var lm_head = try Tensor.init(allocator, lm_head_shape, .f32);
        try initializeLinear(&lm_head);
        
        // Final layer norm
        const norm_shape = Shape.init(&[_]u32{config.hidden_size});
        const norm = try Tensor.ones(allocator, norm_shape, .f32);
        
        return Self{
            .config = config,
            .transformer = transformer,
            .tokenizer = tokenizer,
            .backend = backend,
            .allocator = allocator,
            .embed_tokens = embed_tokens,
            .embed_positions = null,
            .lm_head = lm_head,
            .norm = norm,
        };
    }
    
    /// Free model memory
    pub fn deinit(self: *Self) void {
        self.transformer.deinit();
        self.tokenizer.deinit();
        self.embed_tokens.deinit();
        if (self.embed_positions) |*pos| pos.deinit();
        self.lm_head.deinit();
        self.norm.deinit();
    }
    
    /// Get model information
    pub fn info(self: *const Self) ModelInfo {
        const num_params = self.estimateParameters();
        const memory_usage = self.estimateMemoryUsage();
        
        return ModelInfo{
            .name = "DeepSeek V3",
            .version = "0.1.0",
            .config = self.config,
            .num_parameters = num_params,
            .memory_usage = memory_usage,
        };
    }
    
    /// Generate text completion
    pub fn generate(self: *Self, input_tokens: []const u32, max_tokens: u32) ![]u32 {
        _ = self;
        _ = input_tokens;
        _ = max_tokens;
        
        // TODO: Implement actual generation
        // This would involve:
        // 1. Run forward pass through transformer layers
        // 2. Apply final layer norm and output projection
        // 3. Sample next token from logits
        // 4. Repeat until max_tokens or EOS
        
        std.log.debug("Generation not yet implemented");
        return error.NotImplemented;
    }
    
    /// Forward pass through the model
    pub fn forward(
        self: *Self,
        input_ids: []const u32,
        output: *Tensor,
    ) !void {
        // TODO: Implement forward pass
        // 1. Embedding lookup
        // 2. Transformer forward pass
        // 3. Final layer norm
        // 4. Language model head
        
        _ = self;
        _ = input_ids;
        _ = output;
        
        std.log.debug("Model forward pass (placeholder)");
    }
    
    /// Estimate model parameters
    fn estimateParameters(self: *const Self) u64 {
        var params: u64 = 0;
        
        // Embedding parameters
        params += @as(u64, self.config.vocab_size) * self.config.hidden_size;
        
        // Transformer parameters (rough estimate)
        const layer_params = @as(u64, self.config.hidden_size) * self.config.hidden_size * 4; // Attention + FFN
        params += layer_params * self.config.num_hidden_layers;
        
        // MoE parameters
        const expert_params = @as(u64, self.config.hidden_size) * self.config.intermediate_size * 2;
        params += expert_params * self.config.num_experts;
        
        // Output head
        params += @as(u64, self.config.hidden_size) * self.config.vocab_size;
        
        return params;
    }
    
    /// Estimate memory usage in bytes
    fn estimateMemoryUsage(self: *const Self) u64 {
        const params = self.estimateParameters();
        const dtype_size: u64 = if (self.config.use_fp16 or self.config.use_bf16) 2 else 4;
        
        // Model weights + activation memory + KV cache
        return params * dtype_size * 2; // Rough estimate
    }
};

// Initialize embedding with small random values
fn initializeEmbedding(tensor: *Tensor) !void {
    const data = try tensor.asSliceF32();
    var rng = std.Random.DefaultPrng.init(42);
    const random = rng.random();
    
    for (data) |*val| {
        val.* = (random.float(f32) - 0.5) * 0.02; // Small random values
    }
}

// Initialize linear layer with Xavier initialization
fn initializeLinear(tensor: *Tensor) !void {
    const data = try tensor.asSliceF32();
    var rng = std.Random.DefaultPrng.init(123);
    const random = rng.random();
    
    const fan_in = tensor.shape.dims[0];
    const fan_out = tensor.shape.dims[1];
    const limit = std.math.sqrt(6.0 / @as(f32, @floatFromInt(fan_in + fan_out)));
    
    for (data) |*val| {
        val.* = (random.float(f32) - 0.5) * 2.0 * limit;
    }
}

// Tests
test "model creation" {
    const testing = std.testing;
    const allocator = testing.allocator;
    
    // Create a dummy backend for testing
    const backend = Backend{
        .type = .cpu,
        .device_id = 0,
        .allocator = allocator,
    };
    
    var model = try Model.loadDefault(allocator, backend);
    defer model.deinit();
    
    const model_info = model.info();
    try testing.expect(model_info.num_parameters > 0);
    try testing.expect(std.mem.eql(u8, model_info.name, "DeepSeek V3"));
}

test "model config" {
    const config = ModelConfig.deepseekV3Default();
    std.testing.expect(config.vocab_size == 129280) catch unreachable;
    std.testing.expect(config.num_experts == 256) catch unreachable;
    std.testing.expect(config.num_experts_per_token == 8) catch unreachable;
} 